# -*- coding: utf-8 -*-
"""Sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GhryczK4WzyBMVdHvS5ahFBRXgzgZRdU

#SENTIMENT ANALYSIS OF COVID-19 TWEETS
"""

#import necessary libraries 
import io
import random
import string
import warnings
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')




import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import words
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *


# sklearn imports
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics


# python imports
import re
import json
import os
from collections import Counter
import datetime as dt


# Visualization
from matplotlib import pyplot as plt
from matplotlib import ticker
import seaborn as sns
from sklearn import feature_extraction, linear_model, model_selection, preprocessing
from wordcloud import WordCloud
from tqdm import tqdm_notebook


# Saving models
import pickle

df = pd.read_csv("/content/ Coronavirus_Tweets.csv", encoding = 'latin-1')

df.head()

df.tail()

df.describe()

df.columns

df.isnull()

df.isnull().values.any()
df.isnull().sum()

"""We drop missing values columns"""

df.drop(['reply_to_status_id', 'reply_to_user_id', 'reply_to_screen_name', 'country_code',	'place_full_name', 'place_type', 'account_lang', 'screen_name','source','is_quote','is_retweet',	'favourites_count',	'retweet_count',	'followers_count',	'friends_count'], axis = 1, inplace = True)

df.head()

df['lang'].value_counts()[ :10]

plt.figure(figsize = (15,7))
sns.barplot(x = df['lang'].value_counts()[:10].index , y = df['lang'].value_counts()[:10])
plt.xlabel('Language', fontsize = 20)
plt.ylabel('Percentage of Tweets', fontsize = 20)
plt.xticks(fontsize = 15)
plt.title('Top Ten Languages with most Tweets', fontsize=20)
plt.show()

new_df = df[df['lang']=='en']
new_df.head()

new_df1 = new_df.reset_index(inplace = True)
new_df.tail()

txt = new_df['text']
txt.head()

#Removing URLs, hashtags from tweets
txt_ru = txt.apply(lambda x: re.sub(r"@\S+", "", str(x)))
txt_ru = txt_ru.apply(lambda x: re.sub(r"https\S+", "", str(x)))
txt_rh = txt_ru.apply(lambda x: re.sub(r"#\S+", "",str(x)))
txt_rh.head()

#converting tweets to lowercase
txt_lower = txt_rh.apply(lambda x: x.lower())
txt_lower.head()

#removing punctuations
txt_rp = txt_lower.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
txt_rp.head()

#remove special characters
txt_rs = txt_rp.apply(lambda x: re.sub('[^a-zA-Z0-9]', ' ', str(x)))
txt_rs.head()

nltk.download("stopwords")

#Removing stopwords
stop_words = set(stopwords.words('english'))
#stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19'])
txt_rstpw = txt_rs.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
txt_rstpw.head()

tweets = txt_rstpw.to_frame()
tweets.head()

#concat tweets into list of words
word_list = [word for line in txt_rstpw for word in line.split()]
word_list[4:15]

from wordcloud import STOPWORDS

wordcloud = WordCloud(background_color='black', stopwords=STOPWORDS, height = 3600, width = 9600).generate(' '.join(word_list))


plt.figure(figsize = (12, 10), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud.png')
plt.show()

nltk.download("vader_lexicon")

sid = SentimentIntensityAnalyzer()
sentiment_scores = txt_rstpw.apply(lambda x: sid.polarity_scores(x))
sent_scores_df = pd.DataFrame(list(sentiment_scores))
sent_scores_df.head()

tweets["sentiment"] = sent_scores_df['compound']
tweets.head()

new_df.drop(['index','verified', 'lang','text', 'account_created_at'], axis =1,inplace = True)

new_df['clean_text'] = tweets['text']

new_df.head()

new_df["Polarity"] = sent_scores_df['compound']
new_df.head()

def Polarity_score(data):
  if data['Polarity'] == 0:
    return 'Neu'
  elif data["Polarity"] > 0:
    return 'Pos'
  else:
    return 'Neg'

new_df['sentiment'] = new_df.apply(Polarity_score, axis=1)
new_df.head(10)

new_dt = new_df.sample(n = 1500)
new_dt.shape

new_dt.head(10)

new_dt.tail(10)

new_dt["Date"] = pd.to_datetime(df["created_at"]).dt.date
new_dt["Time"] = pd.to_datetime(df["created_at"]).dt.time

new_dt.head()

new_dt["sentiment"].value_counts()

new_dt.to_csv("Data.csv", index = False)

from google.colab import files
files.download("Data.csv")

pos_words = ' '.join([text for text in new_df['clean_text'][new_df['sentiment'] == 'Pos']])

wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(pos_words)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

neg_words = ' '.join([text for text in new_df['clean_text'][new_df['sentiment'] == 'Neg']])

wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_words)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

neut_words = ' '.join([text for text in new_df['clean_text'][new_df['sentiment'] == 'Neu']])

wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neut_words)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# bag-of-words feature matrix
from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(stop_words='english')

X = bow_vectorizer.fit_transform(txt_rstpw)
y = new_df["sentiment"]

X.shape

y.shape

#splitting the data into train set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)

classifier = MultinomialNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

print("Test accuracy: ",classifier.score(X_test, y_test))
print("Training accuracy: ",classifier.score(X_train, y_train))

#on CountVectorizer
from sklearn.svm import LinearSVC
LSVC = LinearSVC()

LSVC.fit(X_train, y_train)
y_pred = LSVC.predict(X_test)
print("Test accuracy: ",LSVC.score(X_test, y_test))
print("Training accuracy: ",LSVC.score(X_train, y_train))

from sklearn.tree import DecisionTreeClassifier
DTC = DecisionTreeClassifier()

DTC.fit(X_train, y_train)
y_pred = DTC.predict(X_test)

print("Test accuracy: ",DTC.score(X_test, y_test))
print("Training accuracy: ",DTC.score(X_train, y_train))

# TF-IDF feature matrix
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

X = tfidf_vectorizer.fit_transform(txt_rstpw)
y = new_df["sentiment"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)

classifier = MultinomialNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
print("Test accuracy: ",classifier.score(X_test, y_test))
print("Training accuracy: ",classifier.score(X_train, y_train))

#on tf-idf
from sklearn.svm import LinearSVC
LSVC = LinearSVC()

LSVC.fit(X_train, y_train)
y_pred = LSVC.predict(X_test)
print("Test accuracy: ",LSVC.score(X_test, y_test))
print("Training accuracy: ",LSVC.score(X_train, y_train))

from sklearn.tree import DecisionTreeClassifier
DTC = DecisionTreeClassifier()
DTC.fit(X_train, y_train)
y_pred = DTC.predict(X_test)

print("Test accuracy: ",DTC.score(X_test, y_test))
print("Training accuracy: ",DTC.score(X_train, y_train))

